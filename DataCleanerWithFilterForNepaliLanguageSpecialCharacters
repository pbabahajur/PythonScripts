import jsonlines
import re
import random
import asyncio
import time
import json
import logging
import os
from langdetect import detect, LangDetectException
from nltk.tokenize import sent_tokenize
from collections import Counter
from typing import List, Dict
import nltk

nltk.download('punkt')

logging.basicConfig(level=logging.INFO,
                    format='%(levelname)s: %(message)s',
                    filename='preprocess.log',
                    filemode='w')
logger = logging.getLogger(__name__)

UI_PHRASES = {
    "add to cart", "buy now", "click here", "read more", "sign up", "subscribe",
    "free trial", "learn more", "checkout", "order now", "view details"
}
SCRAPER_FAILURES = {"title", "heading", "null", "n/a", "loading...", "error", "undefined"}

LOWER_WORD_THRESHOLD = 6
UPPER_WORD_THRESHOLD = 400

def remove_urls(text: str) -> str:
    return re.sub(r'http[s]?://\S+', '', text)

def clean_text(text: str) -> str:
    text = remove_urls(text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def clean_special_characters(text: str) -> str:
    return re.sub(
        r'[^\x00-\x7F\u0900-\u097F\u0980-\u09FF\u0A00-\u0A7F\u0B00-\u0B7F\u0B80-\u0BFF\u0C00-\u0C7F\u2000-\u206Fредрее]',
        '', 
        text
    )

def remove_repeated_words(text: str) -> str:
    return re.sub(r'\b(\w+)\s+\1\b', r'\1', text, flags=re.UNICODE)

def remove_leading_numbers(text: str) -> str:
    return re.sub(r'^\d+\s*', '', text)

def segment_sentences(text: str) -> List[str]:
    sentences = sent_tokenize(text)
    language_grouped_sentences = []
    temp_group = []
    current_language = None
    for sentence in sentences:
        try:
            detected_lang = detect(sentence)
            if current_language is None:
                current_language = detected_lang
            elif detected_lang != current_language:
                language_grouped_sentences.append(temp_group)
                temp_group = [sentence]
                current_language = detected_lang
            else:
                temp_group.append(sentence)
        except LangDetectException:
            continue
    if temp_group:
        language_grouped_sentences.append(temp_group)
    return language_grouped_sentences

def remove_noise(sentences: List[str]) -> List[str]:
    clean_sentences = []
    for sentence in sentences:
        if len(sentence.split()) > 2 and len(set(sentence)) > 3:
            clean_sentences.append(sentence)
    return clean_sentences

def deduplicate(texts: List[str]) -> List[str]:
    return list(set(texts))

def generate_context(target: str) -> str:
    templates = [
        f"This is a definition: {target}",
        f"The following statement is true: {target}",
        f"Here is some information: {target}",
        f"In this context, {target} is the answer.",
        f"The explanation for this is: {target}"
    ]
    return random.choice(templates)

def truncate_context(context: str, answer: str, threshold: float = 0.5) -> str:
    context_words = context.split()
    answer_words = answer.split()
    if not context_words:
        return context
    common = sum(1 for word in context_words if word.lower() in {w.lower() for w in answer_words})
    if common / len(context_words) > threshold and len(context_words) > 6:
        return ' '.join(context_words[:3] + context_words[-3:])
    return context

def is_target_double_size(input_text: str, target_value: str) -> bool:
    return len(target_value) >= 2 * len(input_text)

def has_minimum_word_count(text: str, min_words: int = LOWER_WORD_THRESHOLD) -> bool:
    return len(text.split()) >= min_words

def is_valid_question(text: str) -> bool:
    return text.strip().endswith('?')

def is_overly_redundant(input_text: str, answer: str, threshold: float = 0.6) -> bool:
    input_words = set(input_text.lower().split())
    answer_words = answer.lower().split()
    if not answer_words:
        return True
    common = sum(1 for word in answer_words if word in input_words)
    ratio = common / len(answer_words)
    return ratio >= threshold

def should_include_answer(answer: str) -> bool:
    if answer.strip().endswith(('.', 'ред')):
        return True
    word_count = len(answer.split())
    unwanted_phrases = UI_PHRASES.union(SCRAPER_FAILURES)
    if word_count > 200 and not any(phrase in answer.lower() for phrase in unwanted_phrases):
        return True
    return False

def has_forbidden_words(answer: str) -> bool:
    forbidden_words = ['save', 'pdf', 'download']
    regex = re.compile(r'\b\w+\b', re.UNICODE)
    word_count = Counter(regex.findall(answer.lower()))
    return sum(word_count[word] for word in forbidden_words) >= 2

def load_json_data(filename: str) -> List[dict]:
    data = []
    control_char_regex = re.compile(r'[\x00-\x08\x0B\x0C\x0E-\x1F]')
    
    with open(filename, 'r', encoding='utf-8') as f:
        for line_number, line in enumerate(f, 1):
            try:
                cleaned_line = control_char_regex.sub('', line)
                entry = json.loads(cleaned_line)
                if 'input' in entry and ('value' in entry or 'target' in entry):
                    data.append(entry)
                else:
                    logger.warning(f"Skipping invalid entry at line {line_number}")
            except Exception as e:
                logger.error(f"Error at line {line_number}: {str(e)}")
    return data

def save_incrementally_to_json(data: List[dict], filename: str):
    try:
        if os.path.exists(filename):
            with open(filename, 'r+', encoding='utf-8') as f:
                try:
                    existing = json.load(f)
                except json.JSONDecodeError:
                    existing = []
                existing.extend(data)
                f.seek(0)
                json.dump(existing, f, indent=4, ensure_ascii=False)
                f.truncate()
        else:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=4, ensure_ascii=False)
    except Exception as e:
        logger.error(f"Failed to save data: {str(e)}")

async def process_data_chunk(chunk: List[dict], output_file: str) -> int:
    processed = []
    for entry in chunk:
        try:
            input_val = entry['input']
            target_val = entry.get('value') or entry.get('target')
            
            # Clean and process input
            input_val = clean_text(input_val.replace('\n', ' '))
            input_val = clean_special_characters(input_val)
            input_val = remove_repeated_words(input_val)
            input_val = remove_leading_numbers(input_val)
            
            # Clean and process target
            target_val = clean_text(target_val.replace('\n', ' '))
            target_val = clean_special_characters(target_val)
            target_val = remove_repeated_words(target_val)
            target_val = remove_leading_numbers(target_val)
            
            if not target_val.strip():
                continue

            # Main processing logic
            if len(target_val.split()) > 200:
                if len(input_val.split()) < LOWER_WORD_THRESHOLD:
                    input_val = f"{input_val} {' '.join(target_val.split()[:5])}"
                if not is_valid_question(input_val):
                    input_val = f"What is {input_val}?"
                    
            if all([
                has_minimum_word_count(input_val),
                is_target_double_size(input_val, target_val),
                should_include_answer(target_val),
                not has_forbidden_words(target_val),
                not is_overly_redundant(input_val, target_val)
            ]):
                context = truncate_context(generate_context(target_val), target_val)
                processed.append({
                    "question": input_val.strip('?') + '?',
                    "context": context,
                    "answer": target_val
                })
                
        except Exception as e:
            logger.error(f"Error processing entry: {str(e)}")
    
    if processed:
        save_incrementally_to_json(processed, output_file)
    return len(processed)

async def process_data_in_parallel(data: List[dict], output_file: str, chunk_size: int = 50) -> int:
    tasks = []
    for i in range(0, len(data), chunk_size):
        tasks.append(process_data_chunk(data[i:i+chunk_size], output_file))
    results = await asyncio.gather(*tasks)
    return sum(results)

async def preprocess_data(input_file: str, output_file: str = "qa_dataset.json"):
    logger.info("Starting data preprocessing")
    data = load_json_data(input_file)
    total_processed = await process_data_in_parallel(data, output_file)
    logger.info(f"Processed {total_processed} valid entries")

if __name__ == "__main__":
    start = time.time()
    asyncio.run(preprocess_data("output_data/data_part_1.jsonl"))
    logger.info(f"Completed in {time.time()-start:.2f} seconds")
